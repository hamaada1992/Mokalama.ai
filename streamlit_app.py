import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "poll"

import streamlit as st
import tempfile
import pandas as pd
import plotly.express as px
import json
from faster_whisper import WhisperModel
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import time
import re
import numpy as np

# ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¸Ù‡Ø± Ø§Ù„Ø¹Ø§Ù…
st.set_page_config(
    page_title="ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ",
    layout="wide",
    page_icon="ğŸ“",
    initial_sidebar_state="expanded"
)

# ØªØ®ØµÙŠØµ Ø§Ù„Ø£Ù„ÙˆØ§Ù†
st.markdown("""
<style>
    :root {
        --primary: #3498db;
        --secondary: #2ecc71;
        --danger: #e74c3c;
        --warning: #f39c12;
        --dark: #2c3e50;
    }
    
    .st-emotion-cache-1y4p8pa {
        background-color: #f8f9fa;
    }
    
    .stAlert {
        border-left: 4px solid var(--danger);
        border-radius: 4px;
    }
    
    .critical-call {
        border: 2px solid var(--danger);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        background-color: #fff8f8;
    }
    
    .header {
        color: var(--dark);
        border-bottom: 2px solid var(--primary);
        padding-bottom: 10px;
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“ ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")

# ========== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ==========
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
model_size = st.sidebar.radio(
    "Ø­Ø¬Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª",
    ["tiny", "base"], 
    index=1,
    help="Ø§Ø®ØªØ± tiny Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø±Ø¹ Ø£Ùˆ base Ù„Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰"
)

# ========== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ==========
@st.cache_resource(show_spinner=False)
def load_whisper_model(size):
    st.info(f"â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª ({size})...")
    return WhisperModel(size, device="cpu", compute_type="int8")

whisper_model = load_whisper_model(model_size)

@st.cache_resource(show_spinner=False)
def load_sentiment_model():
    model_name = "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        return pipeline(
            "sentiment-analysis", 
            model=model, 
            tokenizer=tokenizer,
            truncation=True,
            max_length=512
        )
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {str(e)}")
        st.stop()

sentiment_pipeline = load_sentiment_model()

# ========== ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ==========
corrections = {
    "Ø§Ù„ÙØªÙˆØ±": "Ø§Ù„ÙØ§ØªÙˆØ±Ø©",
    "Ø²ÙŠØ§Ø¯": "Ø²ÙŠØ§Ø¯Ø©",
    "Ø§Ù„Ù„ÙŠØ²ÙˆÙ…": "Ø§Ù„Ù„Ù‘Ø²ÙˆÙ…",
    "Ø§Ù„Ù…ØµØ§Ø¯Ø©": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©",
    "Ø¨Ø¯ÙŠ Ø¨Ø·Ù„": "Ø¨Ø¯ÙŠ Ø£Ø¨Ø¯Ù‘Ù„",
    "Ù…Ø¹ Ø¨ÙˆÙ„": "Ù…Ø¹ Ø¨ÙˆÙ„ÙŠØµØ©",
    "ØªØ§Ø²ÙŠ": "ØªØ§Ø²Ø©",
    "Ø§Ø¯Ø§Ù… Ø§Ù„ÙÙ†ÙŠ": "Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙÙ†ÙŠ",
    "Ø¨Ø§Ù„Ø¯ÙŠ": "Ø¨Ø¯ÙŠ",
    "Ø¥Ø¯Ø§Ù…": "Ø£Ø¯Ø§Ø¡",
    "Ø§Ù„Ø¬ÙˆØ¯ÙŠ": "Ø§Ù„Ø¬ÙˆØ¯Ø©",
    "Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªÙŠ": "ØªÙˆÙ‚Ù‘Ø¹Ø§ØªÙŠ",
    "Ù…Ø¹ Ø·ÙˆØ¨": "Ù…Ø¹Ø·ÙˆØ¨",
    "Ø£Ø¨Ø±ÙŠØ¨Ø§ Ø¯ÙŠÙ„": "Ø£Ø¨ØºÙ‰ Ø¨Ø¯ÙŠÙ„",
    "Ø´Ù†Ù„": "Ø´Ù†Ùˆ",
    "Ø´Ù†": "Ø´Ù†Ùˆ",
    "Ø§Ù„Ø¹Ù„Ø§Ù†": "Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†",
    "Ø§Ù„Ù…ÙˆØ§Ø¶Ù": "Ø§Ù„Ù…ÙˆØ¸Ù‘Ù",
    "Ù…Ù‡Ø¯Ø¨": "Ù…Ù‡Ø°Ù‘Ø¨",
    "Ø§Ù„Ø¶ÙØ¹": "Ø§Ù„Ø¯ÙØ¹",
    "Ø¨ÙŠÙ‚Ø§Ù„": "Ø¨Ø§ÙŠØ¨Ø§Ù„",
    "Ø§Ù„Ù„Ø¹Ø¸Ù…": "Ø§Ù„Ù„Ø§Ø²Ù…",
    "ÙŠÙ†ÙØ¹Ø¯": "ÙŠÙ†ÙØ¹ Ø£Ø¹Ø¯Ù‘Ù„",
    "Ø£Ø¨Ù„ Ù…ÙŠØª Ø´Ù‡Ù„": "Ù‚Ø¨Ù„ Ù…Ø§ ÙŠØªØ´Ø­Ù†",
    "Ø£Ø¨Ù„ Ù…ÙŠØª Ø´Ù‡Ø±": "Ù‚Ø¨Ù„ Ù…Ø§ ÙŠØªØ´Ø­Ù†",
    "Ù„Ø®Ø¨Ø± Ù‡Ùƒ Ù…Ø§ Ø¨Ø³ÙŠØ±": "Ø§Ù„Ø®Ø¨Ø± Ù‡ÙŠÙƒ Ù…Ø§ Ø¨ØµÙŠØ±",
    "ÙŠØ¹ØªÙŠÙƒÙ…": "ÙŠØ¹Ø·ÙŠÙƒÙ…",
    "Ø¹Ù† Ø¬Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„": "Ø¹Ù†Ø¬Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„",
    "ÙˆØ´Ø­Ù† ÙˆØµÙ„Øª": "ÙˆØ§Ù„Ø´Ø­Ù†Ø© ÙˆØµÙ„Øª",
    "Ù…Ø§ ØªØ§Ø¨ÙƒÙˆÙ†": "Ù…ØªÙ‰ Ø¨ÙŠÙƒÙˆÙ†",
    "Ù…Ø§ ØªØ¨ÙƒÙˆÙ†": "Ù…ØªÙ‰ Ø¨ÙŠÙƒÙˆÙ†",
    "ØªØ¨ÙƒÙˆÙ†": "Ø¨ÙŠÙƒÙˆÙ†",
    "Ø¹Ø¶Ø±ÙˆØ±ÙŠ": "Ø¶Ø±ÙˆØ±ÙŠ",
    "Ø¨Ø¯ÙŠ Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ø£Ù… ÙˆØ§Ø­Ø¯ Ù„Ø§ØªÙŠ Ø®Ù…Ø³ Ø³Ø¨Ø¹Ø© ØªØ³Ø¹Ø©": "Ø¨Ø¯ÙŠ Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ù‚Ù… Ù¡Ù£Ù¥Ù§Ù©",
    "Ø§Ù„Ù…Ù†ØªÙƒ": "Ø§Ù„Ù…Ù†ØªØ¬",
    "Ø§Ù„ØªØ­ÙØ©": "Ø±Ø§Ø¦Ø¹",
    "Ù…Ø£Ø¨ÙˆÙ„": "Ù…Ù‚Ø¨ÙˆÙ„",
    "Ù…ÙˆØ§Ø¹Ø¯": "Ù…ÙˆØ¹Ø¯",
    "ØªØ£Ø®Ø± ÙˆØ§Ø¬Ø¯": "ØªØ£Ø®Ù‘Ø± ÙƒØ«ÙŠØ±",
    "Ù‡Ø§ ÙƒØ±Ø±Ù‡Ø§": "Ù‡Ø£ÙƒØ±Ù‘Ø±Ù‡Ø§",
    "Ø¨Ø¯ÙŠØ± Ø¬Ø¹Ù„ Ù…Ù† ØªØ¬": "Ø¨Ø¯ÙŠ Ø£Ø±Ø¬Ù‘Ø¹ Ø§Ù„Ù…Ù†ØªØ¬",
    "ØºÙŠØ· Ù„Ø¨ Ø±Ø£Ù…": "Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ù‚Ù…"
}

# ========== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ==========
TOPIC_KEYWORDS = {
    "ÙÙˆØ§ØªÙŠØ±": ["ÙØ§ØªÙˆØ±Ø©", "Ø¯ÙØ¹", "Ø¨Ø§ÙŠØ¨Ø§Ù„", "Ø§Ù„Ø¯ÙØ¹", "Ø§Ù„ÙØ§ØªÙˆØ±Ø©"],
    "ØªÙˆØµÙŠÙ„": ["ØªÙˆØµÙŠÙ„", "Ø´Ø­Ù†Ø©", "ÙˆØµÙ„Øª", "ØªÙˆØµÙŠÙ„", "Ø§Ù„ØªÙˆØµÙŠÙ„", "Ù…ÙˆØ¹Ø¯ ØªÙˆØµÙŠÙ„", "ØªØ§Ø±ÙŠØ® ØªÙˆØµÙŠÙ„"],
    "Ø§Ø³ØªÙØ³Ø§Ø±": ["Ø§Ø³ØªÙØ³Ø§Ø±", "Ø³Ø¤Ø§Ù„", "Ø§Ø³ØªØ¹Ù„Ø§Ù…"],
    "Ø´ÙƒÙˆÙ‰": ["Ø´ÙƒÙˆÙ‰", "Ù…Ø´ÙƒÙ„Ø©", "Ø§Ø¹ØªØ±Ø§Ø¶", "Ø®Ø·Ø£", "ØºÙ„Ø·"],
    "Ø®Ø¯Ù…Ø© ÙÙ†ÙŠØ©": ["ÙÙ†ÙŠ", "ØªÙ‚Ù†ÙŠ", "ØµÙŠØ§Ù†Ø©", "Ø¥ØµÙ„Ø§Ø­"],
    "Ø¨Ø·Ø§Ù‚Ø©": ["Ø¨Ø·Ø§Ù‚Ø©", "ÙƒØ§Ø±Øª", "Ø§Ø¦ØªÙ…Ø§Ù†", "Ù…Ø¯Ù‰"],
    "ØªØ£Ø®ÙŠØ±": ["ØªØ£Ø®ÙŠØ±", "ØªØ£Ø®Ø±Øª", "Ù…ØªØ£Ø®Ø±Ø©", "ØªØ£Ø®Ø±", "Ø£Ø¨Ø·Ø£"],
    "Ø¥Ù„ØºØ§Ø¡ Ø·Ù„Ø¨": ["Ø¥Ù„ØºØ§Ø¡", "Ø§Ù„ØºÙŠ", "Ø£Ù„ØºÙŠ", "ØªØ±Ø§Ø¬Ø¹", "Ø§Ù„ØºØ§Ø¡", "Ø¥Ù„ØºØ§Ø¡ Ø·Ù„Ø¨"]
}

def detect_topic(text):
    if not text:
        return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    
    text = text.lower()
    topic_counts = {topic: 0 for topic in TOPIC_KEYWORDS}
    
    for topic, keywords in TOPIC_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                topic_counts[topic] += 1
                
    if max(topic_counts.values()) == 0:
        return "Ø£Ø®Ø±Ù‰"
    
    return max(topic_counts, key=topic_counts.get)

# ========== ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø© ==========
def clean_text(text):
    text = re.sub(r"[^\u0600-\u06FF\s]", "", text)
    return re.sub(r"\s+", " ", text).strip()

def manual_correction(text):
    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    return text

def transcribe_audio(path):
    try:
        segments, _ = whisper_model.transcribe(
            path, 
            beam_size=3,
            vad_filter=True,
            language="ar"
        )
        return " ".join([seg.text for seg in segments])[:5000]
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {str(e)}")
        return ""

# ========== ÙˆØ§Ø¬Ù‡Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ==========
uploaded_files = st.file_uploader(
    "ğŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© (WAV, MP3, FLAC)", 
    type=["wav", "mp3", "flac"], 
    accept_multiple_files=True
)

# ========== Ø§Ù„ÙÙ„Ø§ØªØ± ==========
if uploaded_files:
    st.sidebar.header("ğŸ” ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
    sentiment_options = ["Ø³Ù„Ø¨ÙŠ", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯"]
    selected_sentiments = st.sidebar.multiselect(
        "Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
        options=sentiment_options,
        default=sentiment_options
    )
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
    rank_options = ["Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹", "Ø¹Ø§Ù„ÙŠØ©", "Ù…ØªÙˆØ³Ø·Ø©", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", "Ù…Ø­Ø§ÙŠØ¯Ø©"]
    selected_ranks = st.sidebar.multiselect(
        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©",
        options=rank_options,
        default=rank_options
    )

# ========== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª ==========
if uploaded_files:
    st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ {len(uploaded_files)} Ù…ÙƒØ§Ù„Ù…Ø©...")
    start_time = time.time()
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, uploaded_file in enumerate(uploaded_files):
        progress_percent = int((i + 1) / len(uploaded_files) * 100)
        progress_bar.progress(progress_percent)
        status_text.text(f"ğŸ“ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name
            
            call_id = os.path.splitext(uploaded_file.name)[0]
            
            raw = transcribe_audio(tmp_path)
            
            if not raw.strip():
                results.append({
                    "call_id": call_id,
                    "error": "ÙØ´Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ",
                    "text_raw": "",
                    "text_clean": "",
                    "text_corrected": "",
                    "sentiment_label": "error",
                    "sentiment_score": 0.0,
                    "rank": "Error",
                    "topic": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                })
                continue
            
            clean = clean_text(raw)
            corrected = manual_correction(clean)
            topic = detect_topic(corrected)
            
            if not corrected.strip():
                sentiment = {"label": "neutral", "score": 0.0}
                st.warning(f"Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {call_id}: Ø§Ù„Ù†Øµ ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
            else:
                sentiment = sentiment_pipeline(corrected[:512])[0]
            
            label = sentiment["label"]
            score = round(sentiment["score"], 2)
            
            if label == "negative":
                if score > 0.85:
                    rank = "Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹"
                elif score > 0.7:
                    rank = "Ø¹Ø§Ù„ÙŠØ©"
                else:
                    rank = "Ù…ØªÙˆØ³Ø·Ø©"
            elif label == "positive":
                rank = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©"
            else:
                rank = "Ù…Ø­Ø§ÙŠØ¯Ø©"

            results.append({
                "call_id": call_id,
                "text_raw": raw,
                "text_clean": clean,
                "text_corrected": corrected,
                "sentiment_label": label,
                "sentiment_score": score,
                "rank": rank,
                "topic": topic
            })
            
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {uploaded_file.name}: {str(e)}")
            results.append({
                "call_id": uploaded_file.name,
                "error": str(e),
                "text_raw": "",
                "text_clean": "",
                "text_corrected": "",
                "sentiment_label": "error",
                "sentiment_score": 0.0,
                "rank": "Error",
                "topic": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            })
        finally:
            if 'tmp_path' in locals():
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    progress_bar.empty()
    status_text.empty()
    
    end_time = time.time()
    total_time = end_time - start_time
    avg_time = total_time / len(uploaded_files) if uploaded_files else 0
    st.success(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„! Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.1f} Ø«Ø§Ù†ÙŠØ© ({avg_time:.1f} Ø«Ø§Ù†ÙŠØ©/Ù…ÙƒØ§Ù„Ù…Ø©)")

    if results:
        df = pd.DataFrame(results)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
        sentiment_map = {"Ø³Ù„Ø¨ÙŠ": "negative", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ": "positive", "Ù…Ø­Ø§ÙŠØ¯": "neutral"}
        selected_labels = [sentiment_map[s] for s in selected_sentiments]
        
        filtered_df = df[
            df['sentiment_label'].isin(selected_labels) &
            df['rank'].isin(selected_ranks)
        ]
        
        # ========== Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø© ==========
        critical_calls = df[(df['sentiment_label'] == 'negative') & (df['rank'].isin(['Ø¹Ø§Ù„ÙŠØ©', 'Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹']))]
        
        if not critical_calls.empty:
            st.warning(f"ğŸš¨ ØªÙ†Ø¨ÙŠÙ‡: ÙŠÙˆØ¬Ø¯ {len(critical_calls)} Ù…ÙƒØ§Ù„Ù…Ø© Ø³Ù„Ø¨ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ØªØ§Ø¨Ø¹Ø© Ø¹Ø§Ø¬Ù„Ø©!")
            
            for _, row in critical_calls.iterrows():
                with st.expander(f"Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© Ø§Ù„Ø­Ø±Ø¬Ø©: {row['call_id']}", expanded=False):
                    st.markdown(f"**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {row['topic']}")
                    st.markdown(f"**Ø§Ù„Ù…Ø³ØªÙˆÙ‰:** {row['rank']}")
                    st.markdown(f"**Ù†Øµ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©:**")
                    st.write(row['text_corrected'])
        
        # ========== Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ==========
        tab1, tab2, tab3 = st.tabs(["Ø§Ù„Ù†ØªØ§Ø¦Ø¬", "Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©", "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"])
        
        with tab1:
            st.subheader("ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
            st.dataframe(
                filtered_df[["call_id", "topic", "text_corrected", "sentiment_label", "sentiment_score", "rank"]], 
                use_container_width=True,
                height=400
            )
            
            st.subheader("ğŸ” Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©")
            for idx, row in filtered_df.iterrows():
                with st.expander(f"Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©: {row['call_id']} (Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: {row['topic']})", expanded=False):
                    if row['sentiment_label'] == 'negative' and row['rank'] in ['Ø¹Ø§Ù„ÙŠØ©', 'Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹']:
                        st.warning("âš ï¸ Ù…ÙƒØ§Ù„Ù…Ø© Ø³Ù„Ø¨ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© - ØªØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø© Ø¹Ø§Ø¬Ù„Ø©!")
                    
                    st.caption("Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:")
                    st.write(row['text_raw'])
                    st.caption("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ­Ø­:")
                    st.write(row['text_corrected'])
                    st.caption(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {row['sentiment_label']} (Ø«Ù‚Ø©: {row['sentiment_score']:.2f})")
                    st.caption(f"Ø§Ù„Ø£Ù‡Ù…ÙŠØ©: {row['rank']}")
                    st.caption(f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: {row['topic']}")
        
        with tab2:
            st.subheader("ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ")
            
            col1, col2 = st.columns(2)
            with col1:
                fig1 = px.pie(
                    filtered_df, 
                    names="sentiment_label", 
                    title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
                    color_discrete_map={'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}
                )
                st.plotly_chart(fig1, use_container_width=True)
                
                fig3 = px.bar(
                    filtered_df, 
                    x="topic", 
                    color="sentiment_label",
                    title="Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
                    color_discrete_map={'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}
                )
                st.plotly_chart(fig3, use_container_width=True)
                
            with col2:
                fig2 = px.bar(
                    filtered_df, 
                    x="rank", 
                    color="sentiment_label",
                    title="Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø£Ù‡Ù…ÙŠØ©",
                    category_orders={"rank": ["Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹", "Ø¹Ø§Ù„ÙŠØ©", "Ù…ØªÙˆØ³Ø·Ø©", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", "Ù…Ø­Ø§ÙŠØ¯Ø©", "Error"]},
                    color_discrete_map={'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}
                )
                st.plotly_chart(fig2, use_container_width=True)
                
                fig4 = px.treemap(
                    filtered_df, 
                    path=['topic', 'sentiment_label'], 
                    values='sentiment_score',
                    title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±",
                    color='sentiment_label',
                    color_discrete_map={'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}
                )
                st.plotly_chart(fig4, use_container_width=True)
        
        with tab3:
            st.subheader("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ JSON", 
                    json.dumps(results, ensure_ascii=False, indent=2), 
                    file_name="call_results.json", 
                    mime="application/json"
                )
            with col2:
                st.download_button(
                    "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ CSV", 
                    filtered_df.to_csv(index=False).encode("utf-8-sig"), 
                    file_name="call_results.csv", 
                    mime="text/csv"
                )
            
            st.caption("Ù…Ø¹Ø§ÙŠÙ†Ø© JSON:")
            st.json(results[0] if len(results) > 0 else {})
