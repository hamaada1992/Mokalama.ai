import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Fix for PyTorch threading issues
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "poll"  # Fix for file watcher error

import streamlit as st
import tempfile
import pandas as pd
import plotly.express as px
import json
from faster_whisper import WhisperModel
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import time
import re

st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù…", layout="wide")
st.title("ðŸŽ§ ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")

# Model selection
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
model_size = st.sidebar.radio("Ø­Ø¬Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª", ["tiny", "base"], index=1, help="Ø§Ø®ØªØ± tiny Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø±Ø¹ Ø£Ùˆ base Ù„Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰")

@st.cache_resource(show_spinner=False)
def load_whisper_model(size):
    st.info(f"â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª ({size})...")
    return WhisperModel(size, device="cpu", compute_type="int8")

whisper_model = load_whisper_model(model_size)

@st.cache_resource(show_spinner=False)
def load_sentiment_model():
    model_name = "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        return pipeline(
            "sentiment-analysis", 
            model=model, 
            tokenizer=tokenizer,
            truncation=True,
            max_length=512
        )
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {str(e)}")
        st.stop()

sentiment_pipeline = load_sentiment_model()

# Enhanced corrections dictionary
corrections = {
    "Ø§Ù„ÙØªÙˆØ±": "Ø§Ù„ÙØ§ØªÙˆØ±Ø©",
    "Ø²ÙŠØ§Ø¯": "Ø²ÙŠØ§Ø¯Ø©",
    "Ø§Ù„Ù„ÙŠØ²ÙˆÙ…": "Ø§Ù„Ù„Ù‘Ø²ÙˆÙ…",
    "Ø§Ù„Ù…ØµØ§Ø¯Ø©": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©",
    "Ø¨Ø¯ÙŠ Ø¨Ø·Ù„": "Ø¨Ø¯ÙŠ Ø£Ø¨Ø¯Ù‘Ù„",
    "Ù…Ø¹ Ø¨ÙˆÙ„": "Ù…Ø¹ Ø¨ÙˆÙ„ÙŠØµØ©",
    "ØªØ§Ø²ÙŠ": "ØªØ§Ø²Ø©",
    "Ø§Ø¯Ø§Ù… Ø§Ù„ÙÙ†ÙŠ": "Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙÙ†ÙŠ",
    "Ø¨Ø§Ù„Ø¯ÙŠ": "Ø¨Ø¯ÙŠ",
    "Ø¥Ø¯Ø§Ù…": "Ø£Ø¯Ø§Ø¡",
    "Ø§Ù„Ø¬ÙˆØ¯ÙŠ": "Ø§Ù„Ø¬ÙˆØ¯Ø©",
    "Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªÙŠ": "ØªÙˆÙ‚Ù‘Ø¹Ø§ØªÙŠ",
    "Ù…Ø¹ Ø·ÙˆØ¨": "Ù…Ø¹Ø·ÙˆØ¨",
    "Ø£Ø¨Ø±ÙŠØ¨Ø§ Ø¯ÙŠÙ„": "Ø£Ø¨ØºÙ‰ Ø¨Ø¯ÙŠÙ„",
    "Ø´Ù†Ù„": "Ø´Ù†Ùˆ",
    "Ø§Ù„Ø¹Ù„Ø§Ù†": "Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†",
    "Ø§Ù„Ù…ÙˆØ§Ø¶Ù": "Ø§Ù„Ù…ÙˆØ¸Ù‘Ù",
    "Ù…Ù‡Ø¯Ø¨": "Ù…Ù‡Ø°Ù‘Ø¨",
    "Ø§Ù„Ø¶ÙØ¹": "Ø§Ù„Ø¯ÙØ¹",
    "Ø¨ÙŠÙ‚Ø§Ù„": "Ø¨Ø§ÙŠØ¨Ø§Ù„",
    "Ø§Ù„Ù„Ø¹Ø¸Ù…": "Ø§Ù„Ù„Ø§Ø²Ù…",
    "ÙŠÙ†ÙØ¹Ø¯": "ÙŠÙ†ÙØ¹ Ø£Ø¹Ø¯Ù‘Ù„",
    "Ø£Ø¨Ù„ Ù…ÙŠØª Ø´Ù‡Ù„": "Ù‚Ø¨Ù„ Ù…Ø§ ÙŠØªØ´Ø­Ù†",
    "Ù„Ø®Ø¨Ø± Ù‡Ùƒ Ù…Ø§ Ø¨Ø³ÙŠØ±": "Ø§Ù„Ø®Ø¨Ø± Ù‡ÙŠÙƒ Ù…Ø§ Ø¨ØµÙŠØ±",
    "ÙŠØ¹ØªÙŠÙƒÙ…": "ÙŠØ¹Ø·ÙŠÙƒÙ…",
    "Ø¹Ù† Ø¬Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„": "Ø¹Ù†Ø¬Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„",
    "ÙˆØ´Ø­Ù† ÙˆØµÙ„Øª": "ÙˆØ§Ù„Ø´Ø­Ù†Ø© ÙˆØµÙ„Øª",
    "Ù…Ø§ ØªØ§Ø¨ÙƒÙˆÙ†": "Ù…ØªÙ‰ Ø¨ÙŠÙƒÙˆÙ†",
    "Ø¹Ø¶Ø±ÙˆØ±ÙŠ": "Ø¶Ø±ÙˆØ±ÙŠ",
    "ØªØ¨ÙƒÙˆÙ†": "Ø¨ÙŠÙƒÙˆÙ†",
    "Ù…Ø§ ØªØ¨ÙƒÙˆÙ† Ø§Ù„ØªÙˆØµÙŠÙ„": "Ù…ØªÙ‰ Ø¨ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙˆØµÙŠÙ„",
    "Ø¨Ø¯ÙŠ Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ø£Ù… ÙˆØ§Ø­Ø¯ Ù„Ø§ØªÙŠ Ø®Ù…Ø³ Ø³Ø¨Ø¹Ø© ØªØ³Ø¹Ø©": "Ø¨Ø¯ÙŠ Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ù‚Ù… Ù¡Ù£Ù¥Ù§Ù©",
    "Ø§Ù„Ù…Ù†ØªÙƒ": "Ø§Ù„Ù…Ù†ØªØ¬",
    "Ø§Ù„ØªØ­ÙØ©": "Ø±Ø§Ø¦Ø¹",
    "Ù…Ø£Ø¨ÙˆÙ„": "Ù…Ù‚Ø¨ÙˆÙ„",
    "Ù…ÙˆØ§Ø¹Ø¯": "Ù…ÙˆØ¹Ø¯",
    "ØªØ£Ø®Ø± ÙˆØ§Ø¬Ø¯": "ØªØ£Ø®Ù‘Ø± ÙƒØ«ÙŠØ±",
    "Ø´Ù†": "Ø´Ù†Ùˆ",
    "Ù‡Ø§ ÙƒØ±Ø±Ù‡Ø§": "Ù‡Ø£ÙƒØ±Ù‘Ø±Ù‡Ø§",
    "Ø¨Ø¯ÙŠØ± Ø¬Ø¹Ù„ Ù…Ù† ØªØ¬": "Ø¨Ø¯ÙŠ Ø£Ø±Ø¬Ù‘Ø¹ Ø§Ù„Ù…Ù†ØªØ¬",
    "ØºÙŠØ· Ù„Ø¨ Ø±Ø£Ù…": "Ø£Ù„ØºÙŠ Ø·Ù„Ø¨ Ø±Ù‚Ù…"
}

def clean_text(text):
    # Remove non-Arabic characters except spaces
    text = re.sub(r"[^\u0600-\u06FF\s]", "", text)
    # Remove extra spaces
    return re.sub(r"\s+", " ", text).strip()

def manual_correction(text):
    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    return text

def transcribe_audio(path):
    try:
        segments, _ = whisper_model.transcribe(
            path, 
            beam_size=3,  # Faster decoding
            vad_filter=True,  # Remove silence
            language="ar"
        )
        return " ".join([seg.text for seg in segments])[:5000]  # Limit to 5000 characters
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {str(e)}")
        return ""

# File upload section
uploaded_files = st.file_uploader(
    "ðŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© (WAV, MP3, FLAC)", 
    type=["wav", "mp3", "flac"], 
    accept_multiple_files=True
)

if uploaded_files:
    st.info(f"ðŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ {len(uploaded_files)} Ù…ÙƒØ§Ù„Ù…Ø©...")
    start_time = time.time()
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, uploaded_file in enumerate(uploaded_files):
        # Update progress
        progress_percent = int((i + 1) / len(uploaded_files) * 100)
        progress_bar.progress(progress_percent)
        status_text.text(f"ðŸ“ž Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        try:
            # Create temp file
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name
            
            call_id = os.path.splitext(uploaded_file.name)[0]
            
            # Transcription
            raw = transcribe_audio(tmp_path)
            
            # Skip processing if transcription failed
            if not raw.strip():
                results.append({
                    "call_id": call_id,
                    "error": "ÙØ´Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ",
                    "text_raw": "",
                    "text_clean": "",
                    "text_corrected": "",
                    "sentiment_label": "error",
                    "sentiment_score": 0.0,
                    "rank": "Error"
                })
                continue
            
            # Text processing
            clean = clean_text(raw)
            corrected = manual_correction(clean)
            
            # Sentiment analysis (skip if empty)
            if not corrected.strip():
                sentiment = {"label": "neutral", "score": 0.0}
                st.warning(f"Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {call_id}: Ø§Ù„Ù†Øµ ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
            else:
                # Truncate to 512 tokens for model input
                sentiment = sentiment_pipeline(corrected[:512])[0]
            
            # Determine rank
            label = sentiment["label"]
            score = round(sentiment["score"], 2)
            
            # Enhanced ranking system
            if label == "negative":
                if score > 0.85:
                    rank = "Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹"
                elif score > 0.7:
                    rank = "Ø¹Ø§Ù„ÙŠØ©"
                else:
                    rank = "Ù…ØªÙˆØ³Ø·Ø©"
            elif label == "positive":
                rank = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©"
            else:
                rank = "Ù…Ø­Ø§ÙŠØ¯Ø©"

            results.append({
                "call_id": call_id,
                "text_raw": raw,
                "text_clean": clean,
                "text_corrected": corrected,
                "sentiment_label": label,
                "sentiment_score": score,
                "rank": rank
            })
            
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø© {uploaded_file.name}: {str(e)}")
            results.append({
                "call_id": uploaded_file.name,
                "error": str(e),
                "text_raw": "",
                "text_clean": "",
                "text_corrected": "",
                "sentiment_label": "error",
                "sentiment_score": 0.0,
                "rank": "Error"
            })
        finally:
            # Clean up temporary file
            if 'tmp_path' in locals():
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    # Clear progress indicators
    progress_bar.empty()
    status_text.empty()
    
    # Show performance metrics
    end_time = time.time()
    total_time = end_time - start_time
    avg_time = total_time / len(uploaded_files) if uploaded_files else 0
    st.success(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„! Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.1f} Ø«Ø§Ù†ÙŠØ© ({avg_time:.1f} Ø«Ø§Ù†ÙŠØ©/Ù…ÙƒØ§Ù„Ù…Ø©)")

    if results:
        df = pd.DataFrame(results)
        
        # Display results with tabs
        tab1, tab2, tab3 = st.tabs(["Ø§Ù„Ù†ØªØ§Ø¦Ø¬", "Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©", "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"])
        
        with tab1:
            st.subheader("ðŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
            st.dataframe(df[["call_id", "text_corrected", "sentiment_label", "sentiment_score", "rank"]], 
                         use_container_width=True, height=400)
            
            # Detailed view
            st.subheader("ðŸ” Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©")
            for idx, row in df.iterrows():
                with st.expander(f"Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©: {row['call_id']} (Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {row['sentiment_label']})"):
                    st.caption("Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:")
                    st.write(row['text_raw'])
                    st.caption("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ­Ø­:")
                    st.write(row['text_corrected'])
                    st.caption(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {row['sentiment_label']} (Ø«Ù‚Ø©: {row['sentiment_score']:.2f})")
                    st.caption(f"Ø§Ù„Ø£Ù‡Ù…ÙŠØ©: {row['rank']}")
        
        with tab2:
            st.subheader("ðŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ")
            
            col1, col2 = st.columns(2)
            with col1:
                fig1 = px.pie(df, names="sentiment_label", title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
                              color_discrete_map={'positive': 'green', 'negative': 'red', 'neutral': 'blue'})
                st.plotly_chart(fig1, use_container_width=True)
                
                fig3 = px.histogram(df, x="sentiment_score", nbins=20, 
                                   title="ØªÙˆØ²ÙŠØ¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
                                   color="sentiment_label")
                st.plotly_chart(fig3, use_container_width=True)
                
            with col2:
                fig2 = px.bar(df, x="rank", color="rank", 
                             title="ØªØµÙ†ÙŠÙ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª",
                             category_orders={"rank": ["Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹", "Ø¹Ø§Ù„ÙŠØ©", "Ù…ØªÙˆØ³Ø·Ø©", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", "Ù…Ø­Ø§ÙŠØ¯Ø©", "Error"]})
                st.plotly_chart(fig2, use_container_width=True)
                
                fig4 = px.scatter(df, x="sentiment_score", y="rank", color="sentiment_label",
                                title="Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø£Ù‡Ù…ÙŠØ©")
                st.plotly_chart(fig4, use_container_width=True)
        
        with tab3:
            st.subheader("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
            st.caption("ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒØ§Ù…Ù„Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON Ø£Ùˆ CSV")
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "ðŸ“¥ ØªØ­Ù…ÙŠÙ„ JSON", 
                    json.dumps(results, ensure_ascii=False, indent=2), 
                    file_name="call_results.json", 
                    mime="application/json"
                )
            with col2:
                st.download_button(
                    "ðŸ“¥ ØªØ­Ù…ÙŠÙ„ CSV", 
                    df.to_csv(index=False).encode("utf-8-sig"), 
                    file_name="call_results.csv", 
                    mime="text/csv"
                )
            
            # Show sample JSON
            st.caption("Ù…Ø¹Ø§ÙŠÙ†Ø© JSON:")
            st.json(results[0] if len(results) > 0 else {})
