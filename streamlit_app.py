import streamlit as st
import os
import tempfile
import pandas as pd
import plotly.express as px
import json
from faster_whisper import WhisperModel
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import gc
import re

st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù…", layout="wide")
st.title("ğŸ§ ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø©")

@st.cache_resource
def load_whisper_model():
    try:
        model = WhisperModel("medium", device="cpu", compute_type="float32")  # Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper (medium)")
        return model
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper: {str(e)}")
        return None

@st.cache_resource
def load_sentiment_model():
    try:
        model_name = "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        pipe = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")
        return pipe
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {str(e)}")
        return None

whisper_model = load_whisper_model()
sentiment_pipeline = load_sentiment_model()

def clear_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# ØªØ­Ø³ÙŠÙ† ØªØµØ­ÙŠØ­ Ø§Ù„Ù„Ù‡Ø¬Ø§Øª
common_errors = {
    "Ø§Ù„ÙØªÙˆØ±": "Ø§Ù„ÙØ§ØªÙˆØ±Ø©", "Ø²ÙŠØ§Ø¯": "Ø²ÙŠØ§Ø¯Ø©", "Ø§Ù„Ù„ÙŠØ²ÙˆÙ…": "Ø§Ù„Ù„Ø²ÙˆÙ…", "Ø§Ù„Ù…ØµØ§Ø¯Ø©": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©",
    "Ø¨ÙŠÙ‚Ø§Ù„": "Ø¨Ø§ÙŠ Ø¨Ø§Ù„", "Ø§Ù„Ù…ÙˆÙ†": "Ø§Ù„Ù…Ù†ØªØ¬", "Ø³ÙƒØ±Ù‹Ø§": "Ø´ÙƒØ±Ø§Ù‹", "ØªÙ‚Ø²ÙŠÙŠ": "Ø²ÙŠ", "Ø¯ÙŠÙ„": "Ø¨Ø¯ÙŠÙ„",
    "Ø£Ø¨Ø±ÙŠØ¨Ø§": "Ø£Ø¨ØºÙŠ", "Ø·ÙˆØ¨": "Ù…Ø¹Ø·ÙˆØ¨", "Ø§Ù„Ø¹Ù„Ø§Ù†": "Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†", "Ù…Ù‡Ø¯Ø¨": "Ù…Ù‡Ø°Ø¨", "Ø§Ù„Ù…ÙˆØ§Ø¶Ù": "Ø§Ù„Ù…ÙˆØ¸Ù"
}

def normalize_text(text):
    text = re.sub(r"[^\u0600-\u06FF\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def correct_common_errors(text):
    for wrong, right in common_errors.items():
        text = text.replace(wrong, right)
    return text

# ÙƒØ´Ù Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
topics_keywords = {
    "Ø§Ù„Ø¯ÙØ¹": ["Ø¯ÙØ¹", "ÙØ§ØªÙˆØ±Ø©", "Ø¨ÙŠØ¨Ø§Ù„", "Ø¨Ø·Ø§Ù‚Ø©"],
    "Ø§Ù„Ø´Ø­Ù†": ["Ø´Ø­Ù†", "ØªÙˆØµÙŠÙ„", "Ù…ÙˆØ¹Ø¯", "ØªØ£Ø®Ø±"],
    "Ø§Ù„Ø¬ÙˆØ¯Ø©": ["Ø¬ÙˆØ¯Ø©", "Ù…ÙƒØ³ÙˆØ±", "ØªØ§Ù„Ù", "Ù…Ø¹Ø·ÙˆØ¨"],
    "Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹": ["Ø¨Ø¯ÙŠÙ„", "Ø§Ø³ØªØ±Ø¬Ø§Ø¹", "Ø¥Ø±Ø¬Ø§Ø¹"],
    "Ø§Ù„Ø¹Ø±ÙˆØ¶": ["Ø¹Ø±Ø¶", "Ø¹Ø±ÙˆØ¶", "Ø®ØµÙ…"],
    "Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ": ["Ù…ÙˆØ¸Ù", "Ø®Ø¯Ù…Ø©", "Ù…Ù‡Ø°Ø¨", "Ø¯Ø¹Ù…"],
    "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†": ["Ø¹Ù†ÙˆØ§Ù†", "Ù…ÙˆÙ‚Ø¹", "Ù…Ù†Ø·Ù‚ØªÙŠ", "Ø§Ù„Ø±ÙŠØ§Ø¶"]
}

def detect_topic(text):
    scores = {k: sum(text.count(w) for w in ws) for k, ws in topics_keywords.items()}
    return max(scores, key=scores.get) if max(scores.values()) > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

def transcribe_audio(path):
    try:
        segments, _ = whisper_model.transcribe(path, beam_size=5)
        return " ".join([seg.text for seg in segments])
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {str(e)}")
        return ""

uploaded_files = st.file_uploader("ğŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ©", type=["wav", "mp3", "flac"], accept_multiple_files=True)

if uploaded_files and whisper_model and sentiment_pipeline:
    st.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„... Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ù‹Ø§")
    results = []
    with st.spinner("ğŸ” Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø§Ø±ÙŠØ©..."):
        for uploaded_file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp.write(uploaded_file.read())
                tmp_path = tmp.name
            try:
                call_id = os.path.splitext(uploaded_file.name)[0]
                raw = transcribe_audio(tmp_path)
                clean = normalize_text(raw)
                corrected = correct_common_errors(clean)
                topic = detect_topic(corrected)

                if corrected.strip() == "" or len(corrected.split()) < 3:
                    sentiment = {"label": "neutral", "score": 0.5}
                else:
                    sentiment = sentiment_pipeline(corrected)[0]

                label = sentiment["label"]
                score = round(sentiment["score"], 2)
                rank = "High" if label == "negative" and score > 0.8 else "Medium" if label == "negative" else "Low"

                results.append({
                    "call_id": call_id,
                    "text_raw": raw,
                    "text_clean": clean,
                    "text_corrected": corrected,
                    "sentiment_label": label,
                    "sentiment_score": score,
                    "rank": rank,
                    "topic": topic
                })
                os.unlink(tmp_path)
            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù„Ù {uploaded_file.name}: {str(e)}")

    df = pd.DataFrame(results)

    st.subheader("ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    st.dataframe(df[["call_id", "text_corrected", "sentiment_label", "sentiment_score", "rank", "topic"]], use_container_width=True)

    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(px.pie(df, names="sentiment_label", title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"), use_container_width=True)
    with col2:
        st.plotly_chart(px.histogram(df, x="topic", title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹"), use_container_width=True)

    st.subheader("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    st.download_button("ğŸ“¥ JSON", json.dumps(results, ensure_ascii=False, indent=2), file_name="call_results.json", mime="application/json")
    st.download_button("ğŸ“¥ CSV", df.to_csv(index=False).encode("utf-8-sig"), file_name="call_results.csv", mime="text/csv")

    clear_memory()

else:
    st.warning("ğŸ“‚ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© ÙˆÙˆØ¬ÙˆØ¯ Ø§ØªØµØ§Ù„ Ø¬ÙŠØ¯ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.")
